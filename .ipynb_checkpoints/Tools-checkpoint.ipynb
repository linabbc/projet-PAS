{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "\n",
    "*Ce notebook sert principalement à définir des fonctions et outils que nous allons utiliser au long de ce projet*\n",
    "\n",
    "**le 20/11/21**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import *\n",
    "\n",
    "def preprocessing(questions):\n",
    "    \"\"\"Retourne un dataframe où les questions ont subies les différentes étapes de préprocessing:\n",
    "    \n",
    "    - Suppression des majuscules.\n",
    "    \n",
    "    - Suppression des accents.\n",
    "    \n",
    "    - Tokenisation ( à l'aide de la regex suivante : [a-zA-Z]+ seules les mots sont conservés).\n",
    "    \n",
    "    - Suppression des stops words en français.\n",
    "    \n",
    "    - Stemmatisation \n",
    "    \n",
    "    questions -- Série pandas comprenant les questions que l'on souhaite utiliser comme référence\n",
    "    pour le chatbot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nous commençons par retirer les accents et les majuscules sur les mots.\n",
    "    questions = questions.apply(lambda sentence : sentence.lower())\n",
    "    questions = questions.apply(lambda sentence : unidecode(sentence))\n",
    "    # Pour les questions, nous ne gardons que les mots, nous supprimons tous le reste.\n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")\n",
    "    questions = questions.apply(lambda sentence : tokenizer.tokenize(sentence))\n",
    "    # On retire les stops words.\n",
    "    stops = set(stopwords.words(\"french\"))\n",
    "    questions = questions.apply(lambda words : [word for word in words if word not in stops and len(word)>2])\n",
    "    # Nous lemmatisons les mots.\n",
    "    stemmer = FrenchStemmer()\n",
    "    questions = questions.apply(lambda words : [stemmer.stem(word) for word in words])\n",
    "    #lemmer = WordNetLemmatizer()\n",
    "    #questions = questions.apply(lambda words : [lemmer.lemmatize(word) for word in words])\n",
    "    return questions\n",
    "\n",
    "def Vectoriser(tool, data, corpus, to_transform = None):\n",
    "    \"\"\"Retourne du texte vectorisé\n",
    "    \n",
    "    tool -- Un Vectorizer.\n",
    "    \n",
    "    data -- Le dataframe auquel nous rattachons les résultats.\n",
    "    \n",
    "    corpus -- La liste des phrases d'où nous allons piocher les mots\n",
    "    pour la vectorisation.\n",
    "    \n",
    "    to_transform -- la liste des phrases à vectorizer.\n",
    "    \n",
    "    Si to_transform n'est pas rempli, c'est que l'on souhaite rajouter de \n",
    "    nouvelles questions à la liste des questions que l'on possède déjà.\n",
    "    \"\"\"\n",
    "    if to_transform is None:\n",
    "        X = tool.fit_transform(corpus)\n",
    "        vectorised = [list(elem) for elem in X.toarray()]\n",
    "        column = str(tool)[:-2]\n",
    "    else:\n",
    "        tool.fit(corpus)\n",
    "        X = tool.transform(to_transform)\n",
    "        vectorised = [list(elem) for elem in X.toarray()]\n",
    "        column = str(tool)[:-2]\n",
    "    return pd.concat([data, pd.DataFrame({column : vectorised})], axis = 1)\n",
    "\n",
    "def Vectoriser_Question(question, corpus):\n",
    "    \"\"\"Retourne la question qui est posée par l'utilisateur sous forme d'un dataframe\n",
    "    comprenant la question vectorisée de deux manières différentes, à l'aide d'un CountVectorizer\n",
    "    et d'un TfidfVectorizer.\n",
    "    \n",
    "    question -- La question posée par l'utilisateur\n",
    "    \n",
    "    corpus -- La liste des phrases d'où nous allons piocher les mots\n",
    "    pour la vectorisation. \n",
    "    \n",
    "    \"\"\"\n",
    "    question = pd.DataFrame({\"Question_posee\" : [question]})\n",
    "    question = preprocessing(question[\"Question_posee\"])\n",
    "    to_transform = [\" \".join(question[0])]\n",
    "    question = Vectoriser(CountVectorizer(), question, corpus, to_transform)\n",
    "    question = Vectoriser(TfidfVectorizer(), question, corpus, to_transform)\n",
    "    return question\n",
    "\n",
    "def Calcul_Distance(data, question):\n",
    "    \"\"\"Renvoie un dataset auquel on ajoute 4 colonnes:\n",
    "    \n",
    "    Cosinus_CountVectorizer : Cosinus entre le paramètre 'question' et les questions dont nous possedons les réponses qui ont été vectorisé à l'aide d'un CountVectorizer.\n",
    "    \n",
    "    Cosinus_TfidfVectorizer : Cosinus entre le paramètre 'question' et les questions dont nous possedons les réponses qui ont été vectorisé à l'aide d'un TfidfVectorizer.\n",
    "    \n",
    "    Minkowski_CountVectorizer : Distance entre le paramètre 'question' et les questions dont nous possedons les réponses qui ont été vectorisé à l'aide d'un CountVectorizer.\n",
    "    (Avec p = 0.1)\n",
    "    \n",
    "    Monkowski_TfidfVectorizer : Distance entre le paramètre 'question' et les questions dont nous possedons les réponses qui ont été vectorisé à l'aide d'un TfidfVectorizer.\n",
    "    (Avec p = 0.1)\n",
    "    \n",
    "    data -- DataFrame contenant les questions dont on connait les réponses. Celle-ci devant être vectorisées.\n",
    "    \n",
    "    question -- DataFrame contenant la question vectorisée.\n",
    "    \"\"\"\n",
    "    data[\"Cosinus_CountVectorizer\"] = [np.dot(vector, question[\"CountVectorizer\"][0])/(np.linalg.norm(vector)*np.linalg.norm(question[\"CountVectorizer\"][0])) for vector in data[\"CountVectorizer\"]]\n",
    "    data[\"Cosinus_TfidfVectorizer\"] = [np.dot(vector, question[\"TfidfVectorizer\"][0])/(np.linalg.norm(vector)*np.linalg.norm(question[\"TfidfVectorizer\"][0])) for vector in data[\"TfidfVectorizer\"]]\n",
    "    data[\"Minkowski_CountVectorizer\"] = [np.linalg.norm(np.array(vector)-np.array(question[\"CountVectorizer\"][0]), ord=0.1) for vector in data[\"CountVectorizer\"]]\n",
    "    data[\"Minkowski_TfidfVectorizer\"] = [np.linalg.norm(np.array(vector)-np.array(question[\"TfidfVectorizer\"][0]), ord=0.1) for vector in data[\"TfidfVectorizer\"]]\n",
    "    return data\n",
    "\n",
    "def Get_Question_Reponse(data):\n",
    "    \"\"\"Renvoie un DataFrame contenant les questions et réponses les plus problables compte tenu de la question posée.\n",
    "    \n",
    "    data -- DataFrame contenant les questions dont on connait les réponses. Celle-ci devant être vectorisées.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Pour les distance basées sur un calcul d'angle, on prend la valeur la plus grande car plus proche de 1.\n",
    "    response1 = data[data[\"Cosinus_CountVectorizer\"] == max(data[\"Cosinus_CountVectorizer\"])][[\"Question_Origine\", \"Reponse\"]]\n",
    "    response2 = data[data[\"Cosinus_TfidfVectorizer\"] == max(data[\"Cosinus_TfidfVectorizer\"])][[\"Question_Origine\", \"Reponse\"]]\n",
    "    \n",
    "    #Pour les distances basées sur un calcul type Minkowski, on prend le minimum car nous cherchons les vecteurs les plus proches (les moins loins).\n",
    "    response3 = data[data[\"Minkowski_CountVectorizer\"] == min(data[\"Minkowski_CountVectorizer\"])][[\"Question_Origine\", \"Reponse\"]]\n",
    "    response4 = data[data[\"Minkowski_TfidfVectorizer\"] == min(data[\"Minkowski_TfidfVectorizer\"])][[\"Question_Origine\", \"Reponse\"]]\n",
    "    return pd.concat([response1, response2, response3, response4]).drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
